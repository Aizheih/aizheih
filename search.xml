<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>TEST</title>
    <url>/2024/07/12/hello-world/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>通过自反思减轻大型语言模型中的幻觉现象——论文精读</title>
    <url>/2024/07/18/%E9%80%9A%E8%BF%87%E8%87%AA%E5%8F%8D%E6%80%9D%E5%87%8F%E8%BD%BB%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89%E7%8E%B0%E8%B1%A1%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/</url>
    <content><![CDATA[<h1 id="通过自反思减轻大型语言模型中的幻觉现象——论文精读"><a href="#通过自反思减轻大型语言模型中的幻觉现象——论文精读" class="headerlink" title="通过自反思减轻大型语言模型中的幻觉现象——论文精读"></a>通过自反思减轻大型语言模型中的幻觉现象——论文精读</h1><h3 id="Why-研究背景与动机"><a href="#Why-研究背景与动机" class="headerlink" title="Why: 研究背景与动机"></a>Why: 研究背景与动机</h3><h4 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h4><ul>
<li>大型语言模型（LLMs）在生成流畅且有意义的响应方面显示出强大能力，但在实际应用中面临“幻觉”问题，即生成的信息虽然听起来合理但不真实。</li>
<li>这种幻觉现象在医学领域尤为严重，因为错误信息可能会对用户产生严重影响。</li>
<li>当前的医学生成问答系统（GQA）也面临类似问题，尽管在增强信息获取和理解方面显示出潜力。</li>
</ul>
<h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><ul>
<li>调查并解决LLMs在医学生成问答系统中的幻觉问题，以提高生成答案的真实性、一致性和蕴含性。</li>
<li>探索幻觉的发生率及其潜在原因，并提出有效的解决策略，以改善医学问答系统的可靠性和可信度。</li>
</ul>
<h3 id="What-研究内容和方法"><a href="#What-研究内容和方法" class="headerlink" title="What: 研究内容和方法"></a>What: 研究内容和方法</h3><h4 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h4><ul>
<li>分析医学生成问答系统中LLMs的幻觉现象，评估幻觉发生率并探索其潜在原因。</li>
<li>提出了一种<strong>互动自反思方法</strong>，利用LLMs的多轮互动性和多任务能力，通过反馈过程增强生成答案的真实性、一致性和蕴含性。</li>
</ul>
<h4 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h4><ul>
<li>使用五个医学QA数据集进行评估：PubMedQA、MedQuAD、MEDIQA2019、LiveMedQA2017、MASH-QA。</li>
<li>对比评估了五个LLMs生成的答案，包括三个通用LLMs（Vicuna、Alpaca-LoRA、ChatGPT）和两个在医学领域微调的LLMs（MedAlpaca、Robin-medical）。</li>
<li>采用GQA指标（F1和ROUGE-L）评估生成质量，引入Med-NLI评估生成答案的逻辑一致性，使用SciFive模型和CTRL-Eval指标进行进一步评估。</li>
</ul>
<h3 id="How-理论分析与实验验证"><a href="#How-理论分析与实验验证" class="headerlink" title="How: 理论分析与实验验证"></a>How: 理论分析与实验验证</h3><h4 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h4><ul>
<li>提出了一种迭代的内省过程，利用LLMs的多轮互动性和多任务能力，通过自反思方法生成、评估和精炼答案，直至达到满意的事实性水平。</li>
<li>具体方法包括三个循环：<ol>
<li><strong>事实知识获取循环</strong>：模型基于问题生成背景知识，并通过定制和参考自由的评分器进行事实性评估。评估得分低于阈值时，提示模型自我反思并改进知识。</li>
<li><strong>知识一致性回答循环</strong>：模型在生成的背景知识基础上回答问题，并进行一致性评估。如果答案的一致性得分低于阈值，提示模型改进答案。</li>
<li><strong>问题蕴含回答循环</strong>：使用句子-BERT嵌入相似性评估生成答案的蕴含性和可回答性。如果答案未达到满意的蕴含水平，重新开始循环。</li>
</ol>
</li>
</ul>
<h4 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h4><ul>
<li>使用Amazon Mechanical Turk进行人工评估，评估问题一致性和偏离主题的人工评估在样本级别进行，事实一致性的人工评估在句子级别进行。</li>
<li>自动评估结果显示迭代自反思方法在经典重叠指标（如F1和ROUGE-L）和幻觉指标（如MedNLI得分）上表现出色。</li>
<li>人工评估结果与自动评估结果一致，Krippendorff’s alpha测量的注释者间一致性显示出较高的一致性。</li>
<li>进行了消融分析，证明去除精炼步骤或明确的方面描述会导致得分降低，表明这些组件有助于减少幻觉，提高一致性。</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>大型语言模型（LLMs）在实际部署中仍面临“幻觉”问题，即模型生成听起来合理但不真实的信息。这个问题在医学领域尤为严重。本文分析了医学生成问答系统中LLMs的幻觉现象，提出了一种<strong>互动自反思方法</strong>，通过反馈过程增强生成答案的真实性、一致性和蕴含性。实验结果表明，该方法在减少幻觉方面优于基线模型。（用于评估新方法性能的标准或参考模型。在本文中，基线模型包括现有的广泛采用的大型语言模型（如GPT-3、ChatGPT、LLaMA等）和医学问答数据集（如PubMedQA、MedQuAD等）。）</p>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721310320207.png" alt="QQ_1721310320207"></p>
<p>​										图一  幻觉生成问答的一个例子</p>
<h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、<strong>引言</strong></h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 <strong>背景</strong></h3><ul>
<li>LLMs 的“幻觉”问题在医学领域尤其严重</li>
</ul>
<h3 id="1-2-目的"><a href="#1-2-目的" class="headerlink" title="1.2 目的"></a>1.2 <strong>目的</strong></h3><ul>
<li>本研究旨在调查医学GQA系统中LLMs的幻觉现象，尤其是在常见和医学特定的LLMs上。</li>
<li>评估幻觉的发生率，探索潜在原因，并提出减轻这一问题的策略。</li>
</ul>
<h3 id="1-3-方法"><a href="#1-3-方法" class="headerlink" title="1.3 方法"></a>1.3 <strong>方法</strong></h3><ul>
<li>提出了一种迭代的内省过程，利用LLMs的多轮互动性和多任务能力，通过自反思方法生成、评估和精炼答案，直至达到满意的事实性水平。</li>
<li>采用类似的生成-评分-精炼策略，以确保生成的答案与背景知识一致。</li>
</ul>
<h3 id="1-4实验结果"><a href="#1-4实验结果" class="headerlink" title="1.4实验结果"></a>1.4<strong>实验结果</strong></h3><ul>
<li>在不同参数的LLMs（包括7B和175B）和五个医学数据集上的实验结果显示该方法的有效性、可泛化性和可扩展性。</li>
</ul>
<h3 id="1-5贡献"><a href="#1-5贡献" class="headerlink" title="1.5贡献"></a>1.5<strong>贡献</strong></h3><ul>
<li>提供了对医学GQA系统中幻觉现象的全面检查</li>
<li>创新的自反思方法</li>
<li>方法的有效性、可泛化性和可扩展性</li>
</ul>
<h2 id="二、相关工作"><a href="#二、相关工作" class="headerlink" title="二、相关工作"></a><strong>二、相关工作</strong></h2><h3 id="2-1-医学问答系统"><a href="#2-1-医学问答系统" class="headerlink" title="2.1 医学问答系统"></a>2.1 <strong>医学问答系统</strong></h3><ul>
<li>医学QA系统在增强信息获取和理解方面显示出潜力，回答格式包括是&#x2F;否、多项选择、抽取式和生成式等。</li>
<li><strong>预训练语言模型</strong>的引入进一步增强了这些系统的能力，能够生成医学查询响应。</li>
</ul>
<h3 id="2-2-生成问答中的幻觉"><a href="#2-2-生成问答中的幻觉" class="headerlink" title="2.2 生成问答中的幻觉"></a>2.2 <strong>生成问答中的幻觉</strong></h3><ul>
<li>生成真实的医学QA（Faithful GQA）需要严格基于源文本或有效外部知识</li>
<li>术语如语义漂移和事实正确性也可以反映幻觉水平</li>
</ul>
<h3 id="2-3-大型语言模型"><a href="#2-3-大型语言模型" class="headerlink" title="2.3 大型语言模型"></a>2.3 大型语言模型</h3><ul>
<li>优：生成流畅且有意义响应方面的能力</li>
<li>劣：实际应用中仍面临控制、偏见和可靠性方面的挑战，尤其是幻觉问题。</li>
</ul>
<h2 id="三、幻觉分析"><a href="#三、幻觉分析" class="headerlink" title="三、幻觉分析"></a><strong>三、幻觉分析</strong></h2><h3 id="3-1-模型"><a href="#3-1-模型" class="headerlink" title="3.1 模型"></a>3.1 <strong>模型</strong></h3><ul>
<li>评估了五个LLMs生成的答案，包括三个通用LLMs（Vicuna、Alpaca-LoRA、ChatGPT）和两个在医学领域微调的LLMs（MedAlpaca、Robin-medical）。</li>
</ul>
<h3 id="3-2-数据集"><a href="#3-2-数据集" class="headerlink" title="3.2 数据集"></a>3.2 <strong>数据集</strong></h3><ul>
<li>使用五个医学QA数据集进行评估：PubMedQA、MedQuAD、MEDIQA2019、LiveMedQA2017、MASH-QA。</li>
</ul>
<h3 id="3-3-评估协议"><a href="#3-3-评估协议" class="headerlink" title="3.3 评估协议"></a>3.3 评估协议</h3><ul>
<li>使用GQA指标（F1和ROUGE-L）评估生成质量。</li>
<li>引入Med-NLI（Medical Natural Language Inference）评估生成答案与上下文或参考答案的逻辑一致性。</li>
<li>采用了SciFive模型，这是一种在大规模生物医学语料库上预训练的T5模型，评估样本级和句子级的一致性。</li>
<li>使用了CTRL-Eval指标，生成的一致性</li>
</ul>
<h3 id="3-4-结果与讨论"><a href="#3-4-结果与讨论" class="headerlink" title="3.4 结果与讨论"></a>3.4 <strong>结果与讨论</strong></h3><ul>
<li>分析了五个数据集上的250个直接生成的示例，并将问题答案分为三类：事实不一致、查询不一致和偏离主题。<ul>
<li><strong>事实不一致</strong>：提供的信息与事实不一致或相矛盾，例如将Noonan综合征错误地描述为非遗传性。</li>
<li><strong>查询不一致</strong>：答案与查询无关或无意义，例如讨论维生素益处而不提及心脏手术。</li>
<li><strong>偏离主题</strong>：提供的信息相关但未直接回答问题，例如讨论脉络膜但未提及c-Kit在脉络膜黑色素瘤中的作用。</li>
</ul>
</li>
<li>解决这些挑战需要模型具备真实知识调用、上下文理解和推理能力。</li>
</ul>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721309823006.png" alt="QQ_1721309823006"></p>
<p>​									图一  对应代表性例子的有问题答案类别</p>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721311223965.png" alt="QQ_1721311223965"></p>
<p>​									图三  每类问题答案的Google Ngrams频率。</p>
<h2 id="四、幻觉缓解方法"><a href="#四、幻觉缓解方法" class="headerlink" title="四、幻觉缓解方法"></a>四、<strong>幻觉缓解方法</strong></h2><h3 id="4-1-方法论"><a href="#4-1-方法论" class="headerlink" title="4.1 方法论"></a>4.1 <strong>方法论</strong></h3><ul>
<li><p>为了解决幻觉问题，我们提出了一种<strong>迭代自反思</strong>过程，该方法包括三个循环：</p>
<ol>
<li><p><strong>事实知识获取循环</strong>：</p>
<p>模型基于问题生成背景知识，并通过定制和参考自由的评分器进行事实性评估。</p>
<p>评估得分低于阈值时，提示模型自我反思并改进知识。</p>
</li>
<li><p><strong>知识一致性回答循环</strong>：</p>
<p>模型在生成的背景知识基础上回答问题，并进行一致性评估</p>
<p>如果答案的一致性得分低于阈值，提示模型改进答案。</p>
</li>
<li><p><strong>问题蕴含回答循环</strong>：</p>
<p>使用句子-BERT嵌入相似性评估生成答案的蕴含性和可回答性</p>
<p>如果答案未达到满意的蕴含水平，重新开始循环。</p>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721310598033.png" alt="QQ_1721310598033"></p>
</li>
</ol>
</li>
</ul>
<p>​													图四 自反思过程</p>
<h3 id="4-2-实验"><a href="#4-2-实验" class="headerlink" title="4.2 实验"></a>4.2 实验</h3><h4 id="4-2-1-评估"><a href="#4-2-1-评估" class="headerlink" title="4.2.1 评估"></a><strong>4.2.1 评估</strong></h4><ul>
<li>使用Amazon Mechanical Turk进行人工评估</li>
<li><strong>问题一致性和偏离主题的人工评估</strong>：在<strong>样本级别</strong>进行，注释者将每个答案分类为“查询不一致”、“偏离主题”或“蕴含”。</li>
<li><strong>事实一致性的人工评估</strong>：在<strong>句子级别</strong>进行，注释者将每个句子分类为“事实不一致”、“事实一致”或“通用”。</li>
</ul>
<h4 id="4-2-2-结果"><a href="#4-2-2-结果" class="headerlink" title="4.2.2 结果"></a><strong>4.2.2 结果</strong></h4><h5 id="自动评估"><a href="#自动评估" class="headerlink" title="自动评估"></a><strong>自动评估</strong></h5><ul>
<li>表2显示了我们的自反思环节在五个数据集上的自动评估结果。迭代自反思方法在<strong>经典重叠指标</strong>和<strong>幻觉指标</strong>上均表现出色，显著提高了MedNLI得分。</li>
<li>F1和ROUGE-L得分的提高相对较小，主要由于这些指标对黄金答案的依赖性。</li>
</ul>
<h5 id="人工评估"><a href="#人工评估" class="headerlink" title="人工评估"></a><strong>人工评估</strong></h5><ul>
<li><p>表3显示人工评估结果与自动评估结果一致。</p>
</li>
<li><p>Krippendorff’s alpha测量的注释者间一致性显示出较高的一致性，表明注释者在分类问题一致性和事实一致性方面基相同。</p>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721310958210.png"></p>
</li>
</ul>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721311120075.png" alt="QQ_1721311120075"></p>
<p>​										表三和表六  人工评估</p>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721311032810.png" alt="QQ_1721311032810"></p>
<p>​													表四  自动评估</p>
<h3 id="4-3-讨论"><a href="#4-3-讨论" class="headerlink" title="4.3 讨论"></a><strong>4.3 讨论</strong></h3><h4 id="4-3-1-消融研究"><a href="#4-3-1-消融研究" class="headerlink" title="4.3.1 消融研究"></a><strong>4.3.1 消融研究</strong></h4><ul>
<li>为评估具体组件对方法的贡献，进行了消融分析。</li>
<li>数据表示去除精炼步骤、明确的方面描述或准确得分数字都会导致MedNLI和CTRL-Eval得分降低，说明这些组件有助于减少幻觉，提高一致性。</li>
</ul>
<h4 id="4-3-2-案例研究"><a href="#4-3-2-案例研究" class="headerlink" title="4.3.2 案例研究"></a><strong>4.3.2 案例研究</strong></h4><ul>
<li>表5展示了我们的方法在处理事实和查询不一致方面的有效性。例如，直接生成的答案错误地指出EtCO2水平可能不是动脉CO2水平的可靠指标，而我们的答案准确地表明这些测量之间的积极相关性。</li>
</ul>
<p><img src="https://gitee.com/aizheih/image/raw/master/QQ_1721310807482.png" alt="QQ_1721310807482"></p>
<h2 id="五、结论与未来工作"><a href="#五、结论与未来工作" class="headerlink" title="五、结论与未来工作"></a>五、<strong>结论与未来工作</strong></h2><h3 id="5-1-总结"><a href="#5-1-总结" class="headerlink" title="5.1 总结"></a>5.1 <strong>总结</strong></h3><ul>
<li>幻觉问题是生成任务中的重大挑战，尤其是对于AI的可靠性和可信性。</li>
<li>本文系统地研究了医学问答任务中的幻觉问题，并提出了一种<strong>迭代自反思方法</strong>，通过<strong>生成-评分-精炼</strong>策略改进背景知识和答案。</li>
<li>实验结果表明，该方法有效减少了幻觉。</li>
</ul>
<h3 id="5-2-未来工作"><a href="#5-2-未来工作" class="headerlink" title="5.2 未来工作"></a>5.2 <strong>未来工作</strong></h3><ul>
<li>将进一步研究幻觉的潜在原因，探索生成任务中的这一现象。</li>
<li>扩展该方法以应用于其他生成任务，解决这些任务中的挑战。</li>
</ul>
<h2 id="六、个人思考"><a href="#六、个人思考" class="headerlink" title="六、个人思考"></a>六、个人思考</h2><h2 id="6-1-可能的创新方向："><a href="#6-1-可能的创新方向：" class="headerlink" title="6.1 可能的创新方向："></a>6.1 可能的创新方向：</h2><p><strong>多视角自反思</strong>：</p>
<p>引入多个视角进行反思和评估。比如包括模型自身的生成内容、外部知识库和人类反馈等</p>
<p><strong>动态调整权重：</strong></p>
<p>根据不同反馈来源的质量和可信度，动态调整各反馈来源的权重。</p>
<p><strong>可信度增强机制：</strong></p>
<p>可以在自反思的过程中引入可信度增强机制，评估自身答案的可信度</p>
<h3 id="6-2-创新可参考论文："><a href="#6-2-创新可参考论文：" class="headerlink" title="6.2 创新可参考论文："></a>6.2 创新可参考论文：</h3><p><strong>Med-HALT: Medical Domain Hallucination Test for Large Language Models (<a href="https://ar5iv.org/abs/2307.15343">ar5iv</a>)。</strong></p>
<ul>
<li><strong>假设测试（Reasoning Hallucination Tests）</strong>：包括虚假信心测试（False Confidence Test）、“都不是”测试（None of the Above Test）、虚假问题测试（Fake Questions Test），这些测试可以帮助评估模型在逻辑推理和事实准确性方面的能力 。</li>
<li><strong>记忆幻觉测试（Memory Hallucination Tests）</strong>：如摘要到链接测试、PMID到标题测试、标题到链接测试和链接到标题测试，这些测试可以评估模型在信息召回和准确生成方面的能力 (<a href="https://ar5iv.org/abs/2307.15343">ar5iv</a>)。</li>
</ul>
<p><strong>Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI</strong>：</p>
<ul>
<li><strong>可信度增强机制</strong>：引入一种机制，使得模型能够评估其自身生成答案的可信度，并在生成过程中进行调整以减少幻觉现象 ([ACL Anthology](<a href="https://aclanthology.org/2023.conll-1.21.pdf#:~:text=URL%3A">https://aclanthology.org/2023.conll-1.21.pdf#:~:text=URL%3A</a> https%3A%2F%2Faclanthology.org%2F2023.conll))。</li>
</ul>
<p><strong>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models</strong>：</p>
<ul>
<li><strong>多任务学习与知识整合</strong>：通过多任务学习方法，模型可以在不同任务之间共享知识和特征，提高在特定任务中的准确性，并减少幻觉现象 ([ACL Anthology](<a href="https://aclanthology.org/2023.conll-1.21.pdf#:~:text=URL%3A">https://aclanthology.org/2023.conll-1.21.pdf#:~:text=URL%3A</a> https%3A%2F%2Faclanthology.org%2F2023.conll))。</li>
<li><strong>上下文感知生成</strong>：在生成过程中，结合上下文信息，使模型生成的答案更加相关和准确 ([ACL Anthology](<a href="https://aclanthology.org/2023.conll-1.21.pdf#:~:text=URL%3A">https://aclanthology.org/2023.conll-1.21.pdf#:~:text=URL%3A</a> https%3A%2F%2Faclanthology.org%2F2023.conll))。</li>
</ul>
<p><strong>Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Practical Guide</strong>：</p>
<ul>
<li><strong>检索增强生成（RAG）</strong>：结合检索技术，在生成答案之前先检索相关的背景知识，确保生成内容的准确性和相关性 ([ACL Anthology](<a href="https://aclanthology.org/2023.findings-emnlp.123.pdf#:~:text=URL%3A">https://aclanthology.org/2023.findings-emnlp.123.pdf#:~:text=URL%3A</a> https%3A%2F%2Faclanthology.org%2F2023.findings))。</li>
</ul>
<p><strong>Med-HVL: Automatic Medical Domain Hallucination Evaluation for Large Language Models</strong>：</p>
<ul>
<li><strong>自动化幻觉评估工具</strong>：开发一种工具，可以自动评估模型在生成医学内容时的幻觉现象，并提供详细的错误分析 ([ACL Anthology](<a href="https://aclanthology.org/2023.findings-emnlp.123.pdf#:~:text=URL%3A">https://aclanthology.org/2023.findings-emnlp.123.pdf#:~:text=URL%3A</a> https%3A%2F%2Faclanthology.org%2F2023.findings))。</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
</search>
